{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c78eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playwright readability-lxml bs4 asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "# from playwright.async_api import async_playwright\n",
    "from playwright.sync_api import sync_playwright\n",
    "from readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/contents.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb44387",
   "metadata": {},
   "source": [
    "### Sorry but the scrapper is running outside of the notebook because it's easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def uri_validator(x):\n",
    "#     try:\n",
    "#         result = urlparse(x)\n",
    "#         return all([not x.endswith('pdf'), not x.endswith('telecharger'), result.scheme, result.netloc])\n",
    "#     except AttributeError:\n",
    "#         return False\n",
    "\n",
    "# async def fetch_content(url, playwright):\n",
    "#     if not uri_validator(url):\n",
    "#         return None\n",
    "\n",
    "#     browser = await playwright.chromium.launch(headless=False)\n",
    "#     page = await browser.new_page()\n",
    "\n",
    "#     content_type_container = {}\n",
    "\n",
    "#     async def handle_response(response):\n",
    "#         if response.url == url:\n",
    "#             headers = await response.all_headers()\n",
    "#             content_type = headers.get(\"content-type\", \"\")\n",
    "#             content_type_container['value'] = content_type\n",
    "\n",
    "#     page.on(\"response\", handle_response)\n",
    "\n",
    "#     try:\n",
    "#         await page.goto(url, timeout=60000)\n",
    "#         content_type = content_type_container.get('value', '')\n",
    "#         if content_type.startswith(\"application/pdf\"):\n",
    "#             print(f\"[SKIP] Non-HTML content: {content_type} at {url}\")\n",
    "#             return None\n",
    "\n",
    "#         html = await page.content()\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Failed to fetch {url}: {e}\")\n",
    "#         return None\n",
    "#     finally:\n",
    "#         await browser.close()\n",
    "\n",
    "#     doc = Document(html)\n",
    "#     summary_html = doc.summary()\n",
    "#     text = BeautifulSoup(summary_html, 'html.parser').get_text()\n",
    "#     return text.strip()\n",
    "\n",
    "# async def scrape_all(urls):\n",
    "#     async with async_playwright() as playwright:\n",
    "#         tasks = [fetch_content(url, playwright) for url in urls]\n",
    "#         return await asyncio.gather(*tasks)\n",
    "\n",
    "# # Example usage\n",
    "# urls = df['url'].to_list()\n",
    "\n",
    "# results = await scrape_all(urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sicss",
   "language": "python",
   "name": "sicss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
